{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "            Conv2D(64, (5, 5),\n",
    "            padding='same',\n",
    "            input_shape=(28, 28, 1))\n",
    "            )\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train[:, :, :, None]\n",
    "    X_test = X_test[:, :, :, None]\n",
    "    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_on_g = generator_containing_discriminator(g, d)\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    d.trainable = True\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    for epoch in range(1):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = g.predict(noise, verbose=0)\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            d.trainable = False\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            d.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE, nice=False):\n",
    "    g = generator_model()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator_model()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1024, input_dim=100)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 468\n",
      "batch 0 d_loss : 0.712963\n",
      "batch 0 g_loss : 0.690157\n",
      "batch 1 d_loss : 0.693470\n",
      "batch 1 g_loss : 0.687619\n",
      "batch 2 d_loss : 0.671058\n",
      "batch 2 g_loss : 0.675324\n",
      "batch 3 d_loss : 0.666001\n",
      "batch 3 g_loss : 0.667267\n",
      "batch 4 d_loss : 0.649058\n",
      "batch 4 g_loss : 0.659376\n",
      "batch 5 d_loss : 0.624096\n",
      "batch 5 g_loss : 0.642514\n",
      "batch 6 d_loss : 0.605722\n",
      "batch 6 g_loss : 0.631137\n",
      "batch 7 d_loss : 0.589367\n",
      "batch 7 g_loss : 0.617047\n",
      "batch 8 d_loss : 0.568790\n",
      "batch 8 g_loss : 0.608502\n",
      "batch 9 d_loss : 0.555324\n",
      "batch 9 g_loss : 0.600668\n",
      "batch 10 d_loss : 0.543000\n",
      "batch 10 g_loss : 0.586789\n",
      "batch 11 d_loss : 0.527616\n",
      "batch 11 g_loss : 0.580579\n",
      "batch 12 d_loss : 0.509056\n",
      "batch 12 g_loss : 0.567274\n",
      "batch 13 d_loss : 0.498991\n",
      "batch 13 g_loss : 0.555326\n",
      "batch 14 d_loss : 0.490862\n",
      "batch 14 g_loss : 0.552215\n",
      "batch 15 d_loss : 0.488205\n",
      "batch 15 g_loss : 0.542100\n",
      "batch 16 d_loss : 0.478661\n",
      "batch 16 g_loss : 0.538384\n",
      "batch 17 d_loss : 0.466474\n",
      "batch 17 g_loss : 0.527592\n",
      "batch 18 d_loss : 0.465789\n",
      "batch 18 g_loss : 0.526200\n",
      "batch 19 d_loss : 0.454853\n",
      "batch 19 g_loss : 0.517864\n",
      "batch 20 d_loss : 0.457249\n",
      "batch 20 g_loss : 0.516039\n",
      "batch 21 d_loss : 0.458181\n",
      "batch 21 g_loss : 0.508743\n",
      "batch 22 d_loss : 0.456222\n",
      "batch 22 g_loss : 0.508144\n",
      "batch 23 d_loss : 0.449663\n",
      "batch 23 g_loss : 0.504184\n",
      "batch 24 d_loss : 0.443762\n",
      "batch 24 g_loss : 0.507487\n",
      "batch 25 d_loss : 0.448922\n",
      "batch 25 g_loss : 0.506797\n",
      "batch 26 d_loss : 0.446399\n",
      "batch 26 g_loss : 0.501461\n",
      "batch 27 d_loss : 0.453377\n",
      "batch 27 g_loss : 0.506611\n",
      "batch 28 d_loss : 0.456113\n",
      "batch 28 g_loss : 0.507238\n",
      "batch 29 d_loss : 0.455992\n",
      "batch 29 g_loss : 0.501035\n",
      "batch 30 d_loss : 0.452353\n",
      "batch 30 g_loss : 0.500650\n",
      "batch 31 d_loss : 0.457090\n",
      "batch 31 g_loss : 0.510819\n",
      "batch 32 d_loss : 0.454171\n",
      "batch 32 g_loss : 0.510523\n",
      "batch 33 d_loss : 0.455585\n",
      "batch 33 g_loss : 0.511765\n",
      "batch 34 d_loss : 0.458411\n",
      "batch 34 g_loss : 0.519814\n",
      "batch 35 d_loss : 0.465541\n",
      "batch 35 g_loss : 0.518546\n",
      "batch 36 d_loss : 0.469844\n",
      "batch 36 g_loss : 0.525938\n",
      "batch 37 d_loss : 0.473975\n",
      "batch 37 g_loss : 0.523250\n",
      "batch 38 d_loss : 0.472277\n",
      "batch 38 g_loss : 0.529875\n",
      "batch 39 d_loss : 0.469902\n",
      "batch 39 g_loss : 0.536796\n",
      "batch 40 d_loss : 0.468056\n",
      "batch 40 g_loss : 0.532471\n",
      "batch 41 d_loss : 0.471717\n",
      "batch 41 g_loss : 0.538244\n",
      "batch 42 d_loss : 0.475517\n",
      "batch 42 g_loss : 0.553845\n",
      "batch 43 d_loss : 0.475931\n",
      "batch 43 g_loss : 0.558233\n",
      "batch 44 d_loss : 0.474640\n",
      "batch 44 g_loss : 0.559870\n",
      "batch 45 d_loss : 0.475396\n",
      "batch 45 g_loss : 0.566789\n",
      "batch 46 d_loss : 0.474118\n",
      "batch 46 g_loss : 0.577850\n",
      "batch 47 d_loss : 0.489431\n",
      "batch 47 g_loss : 0.583186\n",
      "batch 48 d_loss : 0.480501\n",
      "batch 48 g_loss : 0.592544\n",
      "batch 49 d_loss : 0.474535\n",
      "batch 49 g_loss : 0.601708\n",
      "batch 50 d_loss : 0.475478\n",
      "batch 50 g_loss : 0.606382\n",
      "batch 51 d_loss : 0.470720\n",
      "batch 51 g_loss : 0.614747\n",
      "batch 52 d_loss : 0.468288\n",
      "batch 52 g_loss : 0.633273\n",
      "batch 53 d_loss : 0.474454\n",
      "batch 53 g_loss : 0.639877\n",
      "batch 54 d_loss : 0.477632\n",
      "batch 54 g_loss : 0.649134\n",
      "batch 55 d_loss : 0.466724\n",
      "batch 55 g_loss : 0.666322\n",
      "batch 56 d_loss : 0.464585\n",
      "batch 56 g_loss : 0.672502\n",
      "batch 57 d_loss : 0.466504\n",
      "batch 57 g_loss : 0.677269\n",
      "batch 58 d_loss : 0.461257\n",
      "batch 58 g_loss : 0.693577\n",
      "batch 59 d_loss : 0.465438\n",
      "batch 59 g_loss : 0.699160\n",
      "batch 60 d_loss : 0.459954\n",
      "batch 60 g_loss : 0.720122\n",
      "batch 61 d_loss : 0.473582\n",
      "batch 61 g_loss : 0.728962\n",
      "batch 62 d_loss : 0.467701\n",
      "batch 62 g_loss : 0.742070\n",
      "batch 63 d_loss : 0.456659\n",
      "batch 63 g_loss : 0.752160\n",
      "batch 64 d_loss : 0.452060\n",
      "batch 64 g_loss : 0.754543\n",
      "batch 65 d_loss : 0.448709\n",
      "batch 65 g_loss : 0.775978\n",
      "batch 66 d_loss : 0.446461\n",
      "batch 66 g_loss : 0.776178\n",
      "batch 67 d_loss : 0.446822\n",
      "batch 67 g_loss : 0.803202\n",
      "batch 68 d_loss : 0.436582\n",
      "batch 68 g_loss : 0.802195\n",
      "batch 69 d_loss : 0.442863\n",
      "batch 69 g_loss : 0.821206\n",
      "batch 70 d_loss : 0.438910\n",
      "batch 70 g_loss : 0.834324\n",
      "batch 71 d_loss : 0.440626\n",
      "batch 71 g_loss : 0.837261\n",
      "batch 72 d_loss : 0.433236\n",
      "batch 72 g_loss : 0.852198\n",
      "batch 73 d_loss : 0.432625\n",
      "batch 73 g_loss : 0.853374\n",
      "batch 74 d_loss : 0.430139\n",
      "batch 74 g_loss : 0.867082\n",
      "batch 75 d_loss : 0.426052\n",
      "batch 75 g_loss : 0.870548\n",
      "batch 76 d_loss : 0.414134\n",
      "batch 76 g_loss : 0.874028\n",
      "batch 77 d_loss : 0.415003\n",
      "batch 77 g_loss : 0.895625\n",
      "batch 78 d_loss : 0.403000\n",
      "batch 78 g_loss : 0.890384\n",
      "batch 79 d_loss : 0.390241\n",
      "batch 79 g_loss : 0.896074\n",
      "batch 80 d_loss : 0.384271\n",
      "batch 80 g_loss : 0.910851\n",
      "batch 81 d_loss : 0.378665\n",
      "batch 81 g_loss : 0.924391\n",
      "batch 82 d_loss : 0.376284\n",
      "batch 82 g_loss : 0.934927\n",
      "batch 83 d_loss : 0.387563\n",
      "batch 83 g_loss : 0.931493\n",
      "batch 84 d_loss : 0.362483\n",
      "batch 84 g_loss : 0.941186\n",
      "batch 85 d_loss : 0.353571\n",
      "batch 85 g_loss : 0.952620\n",
      "batch 86 d_loss : 0.361636\n",
      "batch 86 g_loss : 0.959691\n",
      "batch 87 d_loss : 0.346622\n",
      "batch 87 g_loss : 0.985283\n",
      "batch 88 d_loss : 0.348441\n",
      "batch 88 g_loss : 0.963929\n",
      "batch 89 d_loss : 0.345984\n",
      "batch 89 g_loss : 0.971461\n",
      "batch 90 d_loss : 0.344823\n",
      "batch 90 g_loss : 0.994763\n",
      "batch 91 d_loss : 0.333145\n",
      "batch 91 g_loss : 0.983547\n",
      "batch 92 d_loss : 0.319677\n",
      "batch 92 g_loss : 0.988911\n",
      "batch 93 d_loss : 0.310359\n",
      "batch 93 g_loss : 1.006368\n",
      "batch 94 d_loss : 0.311362\n",
      "batch 94 g_loss : 1.009604\n",
      "batch 95 d_loss : 0.297590\n",
      "batch 95 g_loss : 1.017146\n",
      "batch 96 d_loss : 0.314741\n",
      "batch 96 g_loss : 1.007311\n",
      "batch 97 d_loss : 0.299610\n",
      "batch 97 g_loss : 1.027777\n",
      "batch 98 d_loss : 0.295648\n",
      "batch 98 g_loss : 1.011247\n",
      "batch 99 d_loss : 0.316585\n",
      "batch 99 g_loss : 1.019891\n",
      "batch 100 d_loss : 0.309958\n",
      "batch 100 g_loss : 1.028586\n",
      "batch 101 d_loss : 0.267605\n",
      "batch 101 g_loss : 1.031706\n",
      "batch 102 d_loss : 0.263887\n",
      "batch 102 g_loss : 1.035551\n",
      "batch 103 d_loss : 0.282673\n",
      "batch 103 g_loss : 1.036297\n",
      "batch 104 d_loss : 0.266492\n",
      "batch 104 g_loss : 1.035030\n",
      "batch 105 d_loss : 0.252259\n",
      "batch 105 g_loss : 1.039960\n",
      "batch 106 d_loss : 0.277766\n",
      "batch 106 g_loss : 1.043063\n",
      "batch 107 d_loss : 0.286005\n",
      "batch 107 g_loss : 1.032315\n",
      "batch 108 d_loss : 0.270328\n",
      "batch 108 g_loss : 1.022479\n",
      "batch 109 d_loss : 0.271042\n",
      "batch 109 g_loss : 1.038945\n",
      "batch 110 d_loss : 0.257429\n",
      "batch 110 g_loss : 1.002541\n",
      "batch 111 d_loss : 0.248098\n",
      "batch 111 g_loss : 1.004838\n",
      "batch 112 d_loss : 0.248328\n",
      "batch 112 g_loss : 0.997108\n",
      "batch 113 d_loss : 0.264740\n",
      "batch 113 g_loss : 1.010980\n",
      "batch 114 d_loss : 0.274089\n",
      "batch 114 g_loss : 1.011324\n",
      "batch 115 d_loss : 0.288196\n",
      "batch 115 g_loss : 0.968402\n",
      "batch 116 d_loss : 0.240985\n",
      "batch 116 g_loss : 0.966366\n",
      "batch 117 d_loss : 0.236757\n",
      "batch 117 g_loss : 0.965412\n",
      "batch 118 d_loss : 0.258281\n",
      "batch 118 g_loss : 0.937817\n",
      "batch 119 d_loss : 0.230436\n",
      "batch 119 g_loss : 0.940024\n",
      "batch 120 d_loss : 0.260555\n",
      "batch 120 g_loss : 0.907236\n",
      "batch 121 d_loss : 0.259571\n",
      "batch 121 g_loss : 0.925816\n",
      "batch 122 d_loss : 0.244145\n",
      "batch 122 g_loss : 0.904998\n",
      "batch 123 d_loss : 0.249174\n",
      "batch 123 g_loss : 0.902918\n",
      "batch 124 d_loss : 0.231159\n",
      "batch 124 g_loss : 0.910368\n",
      "batch 125 d_loss : 0.215465\n",
      "batch 125 g_loss : 0.895602\n",
      "batch 126 d_loss : 0.224883\n",
      "batch 126 g_loss : 0.895186\n",
      "batch 127 d_loss : 0.228804\n",
      "batch 127 g_loss : 0.906190\n",
      "batch 128 d_loss : 0.224868\n",
      "batch 128 g_loss : 0.897211\n",
      "batch 129 d_loss : 0.239114\n",
      "batch 129 g_loss : 0.860926\n",
      "batch 130 d_loss : 0.240617\n",
      "batch 130 g_loss : 0.870500\n",
      "batch 131 d_loss : 0.253666\n",
      "batch 131 g_loss : 0.865762\n",
      "batch 132 d_loss : 0.242243\n",
      "batch 132 g_loss : 0.884332\n",
      "batch 133 d_loss : 0.254588\n",
      "batch 133 g_loss : 0.866620\n",
      "batch 134 d_loss : 0.227265\n",
      "batch 134 g_loss : 0.885311\n",
      "batch 135 d_loss : 0.233357\n",
      "batch 135 g_loss : 0.853963\n",
      "batch 136 d_loss : 0.253857\n",
      "batch 136 g_loss : 0.899572\n",
      "batch 137 d_loss : 0.299871\n",
      "batch 137 g_loss : 0.826457\n",
      "batch 138 d_loss : 0.259341\n",
      "batch 138 g_loss : 0.875855\n",
      "batch 139 d_loss : 0.266198\n",
      "batch 139 g_loss : 0.816272\n",
      "batch 140 d_loss : 0.235587\n",
      "batch 140 g_loss : 0.832868\n",
      "batch 141 d_loss : 0.237921\n",
      "batch 141 g_loss : 0.839455\n",
      "batch 142 d_loss : 0.246522\n",
      "batch 142 g_loss : 0.824556\n",
      "batch 143 d_loss : 0.247109\n",
      "batch 143 g_loss : 0.822474\n",
      "batch 144 d_loss : 0.266360\n",
      "batch 144 g_loss : 0.841990\n",
      "batch 145 d_loss : 0.271862\n",
      "batch 145 g_loss : 0.813619\n",
      "batch 146 d_loss : 0.273518\n",
      "batch 146 g_loss : 0.823942\n",
      "batch 147 d_loss : 0.280868\n",
      "batch 147 g_loss : 0.842740\n",
      "batch 148 d_loss : 0.273668\n",
      "batch 148 g_loss : 0.861479\n",
      "batch 149 d_loss : 0.273813\n",
      "batch 149 g_loss : 0.811598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 150 d_loss : 0.249867\n",
      "batch 150 g_loss : 0.837692\n",
      "batch 151 d_loss : 0.245821\n",
      "batch 151 g_loss : 0.845293\n",
      "batch 152 d_loss : 0.249444\n",
      "batch 152 g_loss : 0.906833\n",
      "batch 153 d_loss : 0.242098\n",
      "batch 153 g_loss : 0.893135\n",
      "batch 154 d_loss : 0.258660\n",
      "batch 154 g_loss : 0.883668\n",
      "batch 155 d_loss : 0.289212\n",
      "batch 155 g_loss : 0.922704\n",
      "batch 156 d_loss : 0.288055\n",
      "batch 156 g_loss : 0.901016\n",
      "batch 157 d_loss : 0.259395\n",
      "batch 157 g_loss : 0.894757\n",
      "batch 158 d_loss : 0.275787\n",
      "batch 158 g_loss : 0.939145\n",
      "batch 159 d_loss : 0.245256\n",
      "batch 159 g_loss : 0.939180\n",
      "batch 160 d_loss : 0.245505\n",
      "batch 160 g_loss : 0.980663\n",
      "batch 161 d_loss : 0.241653\n",
      "batch 161 g_loss : 0.999423\n",
      "batch 162 d_loss : 0.253789\n",
      "batch 162 g_loss : 1.013940\n",
      "batch 163 d_loss : 0.249566\n",
      "batch 163 g_loss : 1.071289\n",
      "batch 164 d_loss : 0.275921\n",
      "batch 164 g_loss : 1.043800\n",
      "batch 165 d_loss : 0.255303\n",
      "batch 165 g_loss : 1.045381\n",
      "batch 166 d_loss : 0.271136\n",
      "batch 166 g_loss : 1.148969\n",
      "batch 167 d_loss : 0.241880\n",
      "batch 167 g_loss : 1.107743\n",
      "batch 168 d_loss : 0.255795\n",
      "batch 168 g_loss : 1.157859\n",
      "batch 169 d_loss : 0.252984\n",
      "batch 169 g_loss : 1.157499\n",
      "batch 170 d_loss : 0.218750\n",
      "batch 170 g_loss : 1.203403\n",
      "batch 171 d_loss : 0.216565\n",
      "batch 171 g_loss : 1.228080\n",
      "batch 172 d_loss : 0.274692\n",
      "batch 172 g_loss : 1.235544\n",
      "batch 173 d_loss : 0.260273\n",
      "batch 173 g_loss : 1.266101\n",
      "batch 174 d_loss : 0.215894\n",
      "batch 174 g_loss : 1.301358\n",
      "batch 175 d_loss : 0.215084\n",
      "batch 175 g_loss : 1.308311\n",
      "batch 176 d_loss : 0.200955\n",
      "batch 176 g_loss : 1.345133\n",
      "batch 177 d_loss : 0.190178\n",
      "batch 177 g_loss : 1.398875\n",
      "batch 178 d_loss : 0.193154\n",
      "batch 178 g_loss : 1.428627\n",
      "batch 179 d_loss : 0.197936\n",
      "batch 179 g_loss : 1.417959\n",
      "batch 180 d_loss : 0.197693\n",
      "batch 180 g_loss : 1.451280\n",
      "batch 181 d_loss : 0.187201\n",
      "batch 181 g_loss : 1.449636\n",
      "batch 182 d_loss : 0.194448\n",
      "batch 182 g_loss : 1.552787\n",
      "batch 183 d_loss : 0.193265\n",
      "batch 183 g_loss : 1.579856\n",
      "batch 184 d_loss : 0.191154\n",
      "batch 184 g_loss : 1.616047\n",
      "batch 185 d_loss : 0.190084\n",
      "batch 185 g_loss : 1.635612\n",
      "batch 186 d_loss : 0.190977\n",
      "batch 186 g_loss : 1.693064\n",
      "batch 187 d_loss : 0.164826\n",
      "batch 187 g_loss : 1.659322\n",
      "batch 188 d_loss : 0.163088\n",
      "batch 188 g_loss : 1.704655\n",
      "batch 189 d_loss : 0.178323\n",
      "batch 189 g_loss : 1.759735\n",
      "batch 190 d_loss : 0.189355\n",
      "batch 190 g_loss : 1.818005\n",
      "batch 191 d_loss : 0.188167\n",
      "batch 191 g_loss : 1.807918\n",
      "batch 192 d_loss : 0.177607\n",
      "batch 192 g_loss : 1.843542\n",
      "batch 193 d_loss : 0.160428\n",
      "batch 193 g_loss : 1.839081\n",
      "batch 194 d_loss : 0.194147\n",
      "batch 194 g_loss : 1.896875\n",
      "batch 195 d_loss : 0.194161\n",
      "batch 195 g_loss : 1.932178\n",
      "batch 196 d_loss : 0.199069\n",
      "batch 196 g_loss : 1.931195\n",
      "batch 197 d_loss : 0.187079\n",
      "batch 197 g_loss : 1.924697\n",
      "batch 198 d_loss : 0.174133\n",
      "batch 198 g_loss : 2.028861\n",
      "batch 199 d_loss : 0.189607\n",
      "batch 199 g_loss : 1.953745\n",
      "batch 200 d_loss : 0.207081\n",
      "batch 200 g_loss : 2.000448\n",
      "batch 201 d_loss : 0.202865\n",
      "batch 201 g_loss : 1.993940\n",
      "batch 202 d_loss : 0.219480\n",
      "batch 202 g_loss : 2.037112\n",
      "batch 203 d_loss : 0.210006\n",
      "batch 203 g_loss : 2.034867\n",
      "batch 204 d_loss : 0.202011\n",
      "batch 204 g_loss : 1.957644\n",
      "batch 205 d_loss : 0.229827\n",
      "batch 205 g_loss : 1.980203\n",
      "batch 206 d_loss : 0.242583\n",
      "batch 206 g_loss : 1.973991\n",
      "batch 207 d_loss : 0.272624\n",
      "batch 207 g_loss : 1.982169\n",
      "batch 208 d_loss : 0.225328\n",
      "batch 208 g_loss : 1.960309\n",
      "batch 209 d_loss : 0.227276\n",
      "batch 209 g_loss : 1.934155\n",
      "batch 210 d_loss : 0.240242\n",
      "batch 210 g_loss : 1.996659\n",
      "batch 211 d_loss : 0.249912\n",
      "batch 211 g_loss : 1.939507\n",
      "batch 212 d_loss : 0.261338\n",
      "batch 212 g_loss : 1.976369\n",
      "batch 213 d_loss : 0.298560\n",
      "batch 213 g_loss : 1.912419\n",
      "batch 214 d_loss : 0.342537\n",
      "batch 214 g_loss : 1.917063\n",
      "batch 215 d_loss : 0.278036\n",
      "batch 215 g_loss : 1.859138\n",
      "batch 216 d_loss : 0.313528\n",
      "batch 216 g_loss : 1.883382\n",
      "batch 217 d_loss : 0.319569\n",
      "batch 217 g_loss : 1.841852\n",
      "batch 218 d_loss : 0.267298\n",
      "batch 218 g_loss : 1.845453\n",
      "batch 219 d_loss : 0.274276\n",
      "batch 219 g_loss : 1.835755\n",
      "batch 220 d_loss : 0.279538\n",
      "batch 220 g_loss : 1.847219\n",
      "batch 221 d_loss : 0.281632\n",
      "batch 221 g_loss : 1.842949\n",
      "batch 222 d_loss : 0.280986\n",
      "batch 222 g_loss : 1.848068\n",
      "batch 223 d_loss : 0.307051\n",
      "batch 223 g_loss : 1.858221\n",
      "batch 224 d_loss : 0.271785\n",
      "batch 224 g_loss : 1.728022\n",
      "batch 225 d_loss : 0.319976\n",
      "batch 225 g_loss : 1.713286\n",
      "batch 226 d_loss : 0.320345\n",
      "batch 226 g_loss : 1.731387\n",
      "batch 227 d_loss : 0.344404\n",
      "batch 227 g_loss : 1.611907\n",
      "batch 228 d_loss : 0.324545\n",
      "batch 228 g_loss : 1.554755\n",
      "batch 229 d_loss : 0.275558\n",
      "batch 229 g_loss : 1.499479\n",
      "batch 230 d_loss : 0.274087\n",
      "batch 230 g_loss : 1.456962\n",
      "batch 231 d_loss : 0.313999\n",
      "batch 231 g_loss : 1.405368\n",
      "batch 232 d_loss : 0.306917\n",
      "batch 232 g_loss : 1.447950\n",
      "batch 233 d_loss : 0.364550\n",
      "batch 233 g_loss : 1.375221\n",
      "batch 234 d_loss : 0.353285\n",
      "batch 234 g_loss : 1.224114\n",
      "batch 235 d_loss : 0.339572\n",
      "batch 235 g_loss : 1.227484\n",
      "batch 236 d_loss : 0.326791\n",
      "batch 236 g_loss : 1.238304\n",
      "batch 237 d_loss : 0.294553\n",
      "batch 237 g_loss : 1.192406\n",
      "batch 238 d_loss : 0.291104\n",
      "batch 238 g_loss : 1.203012\n",
      "batch 239 d_loss : 0.406702\n",
      "batch 239 g_loss : 1.209018\n",
      "batch 240 d_loss : 0.336003\n",
      "batch 240 g_loss : 1.173607\n",
      "batch 241 d_loss : 0.287553\n",
      "batch 241 g_loss : 1.128010\n",
      "batch 242 d_loss : 0.323615\n",
      "batch 242 g_loss : 1.119511\n",
      "batch 243 d_loss : 0.433488\n",
      "batch 243 g_loss : 1.141852\n",
      "batch 244 d_loss : 0.331490\n",
      "batch 244 g_loss : 1.279983\n",
      "batch 245 d_loss : 0.366675\n",
      "batch 245 g_loss : 1.160882\n",
      "batch 246 d_loss : 0.371405\n",
      "batch 246 g_loss : 1.093148\n",
      "batch 247 d_loss : 0.415447\n",
      "batch 247 g_loss : 0.962791\n",
      "batch 248 d_loss : 0.383301\n",
      "batch 248 g_loss : 0.984290\n",
      "batch 249 d_loss : 0.331724\n",
      "batch 249 g_loss : 1.039696\n",
      "batch 250 d_loss : 0.310684\n",
      "batch 250 g_loss : 1.039970\n",
      "batch 251 d_loss : 0.356227\n",
      "batch 251 g_loss : 1.074145\n",
      "batch 252 d_loss : 0.282242\n",
      "batch 252 g_loss : 1.157243\n",
      "batch 253 d_loss : 0.327847\n",
      "batch 253 g_loss : 1.187294\n",
      "batch 254 d_loss : 0.394479\n",
      "batch 254 g_loss : 1.082448\n",
      "batch 255 d_loss : 0.438648\n",
      "batch 255 g_loss : 1.100096\n",
      "batch 256 d_loss : 0.393936\n",
      "batch 256 g_loss : 1.177675\n",
      "batch 257 d_loss : 0.354932\n",
      "batch 257 g_loss : 0.969026\n",
      "batch 258 d_loss : 0.391641\n",
      "batch 258 g_loss : 1.073485\n",
      "batch 259 d_loss : 0.379533\n",
      "batch 259 g_loss : 1.076857\n",
      "batch 260 d_loss : 0.469260\n",
      "batch 260 g_loss : 0.985558\n",
      "batch 261 d_loss : 0.500008\n",
      "batch 261 g_loss : 0.978730\n",
      "batch 262 d_loss : 0.520934\n",
      "batch 262 g_loss : 0.834366\n",
      "batch 263 d_loss : 0.427263\n",
      "batch 263 g_loss : 0.854993\n",
      "batch 264 d_loss : 0.411965\n",
      "batch 264 g_loss : 0.937159\n",
      "batch 265 d_loss : 0.500111\n",
      "batch 265 g_loss : 1.001125\n",
      "batch 266 d_loss : 0.478142\n",
      "batch 266 g_loss : 1.017645\n",
      "batch 267 d_loss : 0.503111\n",
      "batch 267 g_loss : 0.936963\n",
      "batch 268 d_loss : 0.426557\n",
      "batch 268 g_loss : 1.032982\n",
      "batch 269 d_loss : 0.461586\n",
      "batch 269 g_loss : 0.981220\n",
      "batch 270 d_loss : 0.495543\n",
      "batch 270 g_loss : 1.021405\n",
      "batch 271 d_loss : 0.489989\n",
      "batch 271 g_loss : 1.040576\n",
      "batch 272 d_loss : 0.502626\n",
      "batch 272 g_loss : 1.030482\n",
      "batch 273 d_loss : 0.428743\n",
      "batch 273 g_loss : 1.096272\n",
      "batch 274 d_loss : 0.545471\n",
      "batch 274 g_loss : 1.091349\n",
      "batch 275 d_loss : 0.614964\n",
      "batch 275 g_loss : 1.025877\n",
      "batch 276 d_loss : 0.522271\n",
      "batch 276 g_loss : 0.988162\n",
      "batch 277 d_loss : 0.556479\n",
      "batch 277 g_loss : 1.046440\n",
      "batch 278 d_loss : 0.559407\n",
      "batch 278 g_loss : 0.939329\n",
      "batch 279 d_loss : 0.554061\n",
      "batch 279 g_loss : 1.047421\n",
      "batch 280 d_loss : 0.738390\n",
      "batch 280 g_loss : 0.861258\n",
      "batch 281 d_loss : 0.753668\n",
      "batch 281 g_loss : 0.811033\n",
      "batch 282 d_loss : 0.553203\n",
      "batch 282 g_loss : 0.812859\n",
      "batch 283 d_loss : 0.514111\n",
      "batch 283 g_loss : 0.853371\n",
      "batch 284 d_loss : 0.567862\n",
      "batch 284 g_loss : 0.794161\n",
      "batch 285 d_loss : 0.545813\n",
      "batch 285 g_loss : 0.901848\n",
      "batch 286 d_loss : 0.445873\n",
      "batch 286 g_loss : 1.007000\n",
      "batch 287 d_loss : 0.468255\n",
      "batch 287 g_loss : 1.126052\n",
      "batch 288 d_loss : 0.546621\n",
      "batch 288 g_loss : 1.097482\n",
      "batch 289 d_loss : 0.554975\n",
      "batch 289 g_loss : 1.210008\n",
      "batch 290 d_loss : 0.468561\n",
      "batch 290 g_loss : 1.286015\n",
      "batch 291 d_loss : 0.499714\n",
      "batch 291 g_loss : 1.343130\n",
      "batch 292 d_loss : 0.583709\n",
      "batch 292 g_loss : 1.121897\n",
      "batch 293 d_loss : 0.728311\n",
      "batch 293 g_loss : 1.080267\n",
      "batch 294 d_loss : 0.671088\n",
      "batch 294 g_loss : 0.984095\n",
      "batch 295 d_loss : 0.564255\n",
      "batch 295 g_loss : 0.934475\n",
      "batch 296 d_loss : 0.563721\n",
      "batch 296 g_loss : 0.848322\n",
      "batch 297 d_loss : 0.589941\n",
      "batch 297 g_loss : 0.872811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 298 d_loss : 0.574658\n",
      "batch 298 g_loss : 0.908186\n",
      "batch 299 d_loss : 0.582629\n",
      "batch 299 g_loss : 0.917890\n",
      "batch 300 d_loss : 0.606080\n",
      "batch 300 g_loss : 0.901772\n",
      "batch 301 d_loss : 0.572360\n",
      "batch 301 g_loss : 0.950799\n",
      "batch 302 d_loss : 0.583085\n",
      "batch 302 g_loss : 0.976962\n",
      "batch 303 d_loss : 0.517421\n",
      "batch 303 g_loss : 1.043594\n",
      "batch 304 d_loss : 0.603699\n",
      "batch 304 g_loss : 1.050511\n",
      "batch 305 d_loss : 0.514088\n",
      "batch 305 g_loss : 1.081458\n",
      "batch 306 d_loss : 0.487782\n",
      "batch 306 g_loss : 1.168267\n",
      "batch 307 d_loss : 0.550568\n",
      "batch 307 g_loss : 1.094818\n",
      "batch 308 d_loss : 0.507594\n",
      "batch 308 g_loss : 1.161890\n",
      "batch 309 d_loss : 0.456510\n",
      "batch 309 g_loss : 1.180079\n",
      "batch 310 d_loss : 0.647902\n",
      "batch 310 g_loss : 1.147270\n",
      "batch 311 d_loss : 0.599441\n",
      "batch 311 g_loss : 1.012503\n",
      "batch 312 d_loss : 0.506220\n",
      "batch 312 g_loss : 1.005777\n",
      "batch 313 d_loss : 0.514145\n",
      "batch 313 g_loss : 0.988546\n",
      "batch 314 d_loss : 0.543809\n",
      "batch 314 g_loss : 1.009192\n",
      "batch 315 d_loss : 0.529881\n",
      "batch 315 g_loss : 0.995823\n",
      "batch 316 d_loss : 0.527003\n",
      "batch 316 g_loss : 1.029872\n",
      "batch 317 d_loss : 0.556113\n",
      "batch 317 g_loss : 1.101113\n",
      "batch 318 d_loss : 0.556734\n",
      "batch 318 g_loss : 1.080738\n",
      "batch 319 d_loss : 0.530354\n",
      "batch 319 g_loss : 1.063783\n",
      "batch 320 d_loss : 0.521369\n",
      "batch 320 g_loss : 1.086893\n",
      "batch 321 d_loss : 0.513969\n",
      "batch 321 g_loss : 1.119785\n",
      "batch 322 d_loss : 0.544914\n",
      "batch 322 g_loss : 1.066351\n",
      "batch 323 d_loss : 0.550171\n",
      "batch 323 g_loss : 1.045539\n",
      "batch 324 d_loss : 0.564099\n",
      "batch 324 g_loss : 1.019333\n",
      "batch 325 d_loss : 0.557782\n",
      "batch 325 g_loss : 1.035423\n",
      "batch 326 d_loss : 0.561201\n",
      "batch 326 g_loss : 0.957381\n",
      "batch 327 d_loss : 0.524091\n",
      "batch 327 g_loss : 1.001631\n",
      "batch 328 d_loss : 0.481261\n",
      "batch 328 g_loss : 1.084067\n",
      "batch 329 d_loss : 0.489081\n",
      "batch 329 g_loss : 1.075511\n",
      "batch 330 d_loss : 0.556655\n",
      "batch 330 g_loss : 1.153030\n",
      "batch 331 d_loss : 0.516913\n",
      "batch 331 g_loss : 1.108069\n",
      "batch 332 d_loss : 0.513216\n",
      "batch 332 g_loss : 1.100722\n",
      "batch 333 d_loss : 0.472736\n",
      "batch 333 g_loss : 1.132129\n",
      "batch 334 d_loss : 0.463580\n",
      "batch 334 g_loss : 1.140913\n",
      "batch 335 d_loss : 0.502127\n",
      "batch 335 g_loss : 1.154071\n",
      "batch 336 d_loss : 0.492839\n",
      "batch 336 g_loss : 1.112797\n",
      "batch 337 d_loss : 0.468456\n",
      "batch 337 g_loss : 1.105925\n",
      "batch 338 d_loss : 0.497733\n",
      "batch 338 g_loss : 1.094887\n",
      "batch 339 d_loss : 0.473215\n",
      "batch 339 g_loss : 1.085863\n",
      "batch 340 d_loss : 0.503600\n",
      "batch 340 g_loss : 1.101526\n",
      "batch 341 d_loss : 0.456194\n",
      "batch 341 g_loss : 1.073943\n",
      "batch 342 d_loss : 0.486965\n",
      "batch 342 g_loss : 1.064274\n",
      "batch 343 d_loss : 0.501792\n",
      "batch 343 g_loss : 1.048838\n",
      "batch 344 d_loss : 0.521432\n",
      "batch 344 g_loss : 1.055845\n",
      "batch 345 d_loss : 0.527383\n",
      "batch 345 g_loss : 1.035972\n",
      "batch 346 d_loss : 0.513353\n",
      "batch 346 g_loss : 1.055044\n",
      "batch 347 d_loss : 0.497369\n",
      "batch 347 g_loss : 1.024799\n",
      "batch 348 d_loss : 0.476672\n",
      "batch 348 g_loss : 1.045193\n",
      "batch 349 d_loss : 0.519549\n",
      "batch 349 g_loss : 1.040741\n",
      "batch 350 d_loss : 0.507877\n",
      "batch 350 g_loss : 1.038772\n",
      "batch 351 d_loss : 0.477099\n",
      "batch 351 g_loss : 1.076953\n",
      "batch 352 d_loss : 0.485515\n",
      "batch 352 g_loss : 1.140052\n",
      "batch 353 d_loss : 0.499293\n",
      "batch 353 g_loss : 1.084266\n",
      "batch 354 d_loss : 0.493129\n",
      "batch 354 g_loss : 1.064367\n",
      "batch 355 d_loss : 0.528115\n",
      "batch 355 g_loss : 1.064975\n",
      "batch 356 d_loss : 0.471962\n",
      "batch 356 g_loss : 1.001993\n",
      "batch 357 d_loss : 0.498287\n",
      "batch 357 g_loss : 1.020338\n",
      "batch 358 d_loss : 0.493992\n",
      "batch 358 g_loss : 1.037816\n",
      "batch 359 d_loss : 0.545219\n",
      "batch 359 g_loss : 1.017996\n",
      "batch 360 d_loss : 0.525991\n",
      "batch 360 g_loss : 0.988934\n",
      "batch 361 d_loss : 0.492991\n",
      "batch 361 g_loss : 1.020569\n",
      "batch 362 d_loss : 0.489688\n",
      "batch 362 g_loss : 1.014391\n",
      "batch 363 d_loss : 0.478298\n",
      "batch 363 g_loss : 1.005131\n",
      "batch 364 d_loss : 0.456046\n",
      "batch 364 g_loss : 1.043989\n",
      "batch 365 d_loss : 0.444239\n",
      "batch 365 g_loss : 1.092011\n",
      "batch 366 d_loss : 0.466324\n",
      "batch 366 g_loss : 1.154371\n",
      "batch 367 d_loss : 0.504084\n",
      "batch 367 g_loss : 1.083123\n",
      "batch 368 d_loss : 0.493971\n",
      "batch 368 g_loss : 1.077699\n",
      "batch 369 d_loss : 0.493292\n",
      "batch 369 g_loss : 1.030686\n",
      "batch 370 d_loss : 0.515300\n",
      "batch 370 g_loss : 1.074669\n",
      "batch 371 d_loss : 0.503710\n",
      "batch 371 g_loss : 1.042966\n",
      "batch 372 d_loss : 0.452348\n",
      "batch 372 g_loss : 1.044149\n",
      "batch 373 d_loss : 0.478956\n",
      "batch 373 g_loss : 1.073049\n",
      "batch 374 d_loss : 0.486699\n",
      "batch 374 g_loss : 1.046379\n",
      "batch 375 d_loss : 0.439218\n",
      "batch 375 g_loss : 1.085396\n",
      "batch 376 d_loss : 0.448279\n",
      "batch 376 g_loss : 1.117631\n",
      "batch 377 d_loss : 0.467154\n",
      "batch 377 g_loss : 1.086877\n",
      "batch 378 d_loss : 0.470897\n",
      "batch 378 g_loss : 1.117283\n",
      "batch 379 d_loss : 0.467199\n",
      "batch 379 g_loss : 1.075598\n",
      "batch 380 d_loss : 0.435899\n",
      "batch 380 g_loss : 1.095657\n",
      "batch 381 d_loss : 0.447289\n",
      "batch 381 g_loss : 1.102319\n",
      "batch 382 d_loss : 0.520449\n",
      "batch 382 g_loss : 1.029632\n",
      "batch 383 d_loss : 0.540615\n",
      "batch 383 g_loss : 0.975369\n",
      "batch 384 d_loss : 0.490089\n",
      "batch 384 g_loss : 0.927576\n",
      "batch 385 d_loss : 0.485444\n",
      "batch 385 g_loss : 0.976089\n",
      "batch 386 d_loss : 0.517881\n",
      "batch 386 g_loss : 0.985506\n",
      "batch 387 d_loss : 0.507066\n",
      "batch 387 g_loss : 0.997257\n",
      "batch 388 d_loss : 0.466847\n",
      "batch 388 g_loss : 0.999662\n",
      "batch 389 d_loss : 0.499120\n",
      "batch 389 g_loss : 0.979176\n",
      "batch 390 d_loss : 0.487159\n",
      "batch 390 g_loss : 1.011860\n",
      "batch 391 d_loss : 0.491236\n",
      "batch 391 g_loss : 1.037542\n",
      "batch 392 d_loss : 0.477918\n",
      "batch 392 g_loss : 1.018964\n",
      "batch 393 d_loss : 0.491871\n",
      "batch 393 g_loss : 1.017653\n",
      "batch 394 d_loss : 0.504217\n",
      "batch 394 g_loss : 1.020142\n",
      "batch 395 d_loss : 0.495138\n",
      "batch 395 g_loss : 0.975526\n",
      "batch 396 d_loss : 0.435182\n",
      "batch 396 g_loss : 1.044218\n",
      "batch 397 d_loss : 0.462560\n",
      "batch 397 g_loss : 1.034523\n",
      "batch 398 d_loss : 0.454812\n",
      "batch 398 g_loss : 1.099737\n",
      "batch 399 d_loss : 0.472265\n",
      "batch 399 g_loss : 1.087182\n",
      "batch 400 d_loss : 0.486610\n",
      "batch 400 g_loss : 1.118217\n",
      "batch 401 d_loss : 0.464903\n",
      "batch 401 g_loss : 1.083530\n",
      "batch 402 d_loss : 0.485122\n",
      "batch 402 g_loss : 1.049479\n",
      "batch 403 d_loss : 0.488727\n",
      "batch 403 g_loss : 1.076564\n",
      "batch 404 d_loss : 0.521532\n",
      "batch 404 g_loss : 0.990792\n",
      "batch 405 d_loss : 0.461284\n",
      "batch 405 g_loss : 0.992982\n",
      "batch 406 d_loss : 0.548532\n",
      "batch 406 g_loss : 0.973568\n",
      "batch 407 d_loss : 0.560392\n",
      "batch 407 g_loss : 0.957006\n",
      "batch 408 d_loss : 0.498466\n",
      "batch 408 g_loss : 0.934889\n",
      "batch 409 d_loss : 0.485260\n",
      "batch 409 g_loss : 0.991912\n",
      "batch 410 d_loss : 0.462816\n",
      "batch 410 g_loss : 0.945449\n",
      "batch 411 d_loss : 0.548486\n",
      "batch 411 g_loss : 1.021949\n",
      "batch 412 d_loss : 0.495683\n",
      "batch 412 g_loss : 1.043514\n",
      "batch 413 d_loss : 0.457454\n",
      "batch 413 g_loss : 1.057314\n",
      "batch 414 d_loss : 0.482115\n",
      "batch 414 g_loss : 1.115903\n",
      "batch 415 d_loss : 0.479105\n",
      "batch 415 g_loss : 1.037326\n",
      "batch 416 d_loss : 0.501186\n",
      "batch 416 g_loss : 1.028781\n",
      "batch 417 d_loss : 0.500882\n",
      "batch 417 g_loss : 0.980629\n",
      "batch 418 d_loss : 0.519459\n",
      "batch 418 g_loss : 0.969293\n",
      "batch 419 d_loss : 0.480910\n",
      "batch 419 g_loss : 0.974033\n",
      "batch 420 d_loss : 0.516631\n",
      "batch 420 g_loss : 0.935072\n",
      "batch 421 d_loss : 0.499317\n",
      "batch 421 g_loss : 1.008778\n",
      "batch 422 d_loss : 0.511411\n",
      "batch 422 g_loss : 1.012318\n",
      "batch 423 d_loss : 0.509838\n",
      "batch 423 g_loss : 1.050606\n",
      "batch 424 d_loss : 0.492798\n",
      "batch 424 g_loss : 1.056542\n",
      "batch 425 d_loss : 0.472222\n",
      "batch 425 g_loss : 1.092877\n",
      "batch 426 d_loss : 0.467938\n",
      "batch 426 g_loss : 1.139890\n",
      "batch 427 d_loss : 0.492916\n",
      "batch 427 g_loss : 1.124469\n",
      "batch 428 d_loss : 0.533291\n",
      "batch 428 g_loss : 1.092390\n",
      "batch 429 d_loss : 0.530858\n",
      "batch 429 g_loss : 0.995625\n",
      "batch 430 d_loss : 0.490828\n",
      "batch 430 g_loss : 0.972232\n",
      "batch 431 d_loss : 0.517442\n",
      "batch 431 g_loss : 1.027388\n",
      "batch 432 d_loss : 0.485055\n",
      "batch 432 g_loss : 0.988171\n",
      "batch 433 d_loss : 0.467786\n",
      "batch 433 g_loss : 0.999090\n",
      "batch 434 d_loss : 0.515343\n",
      "batch 434 g_loss : 1.082131\n",
      "batch 435 d_loss : 0.466761\n",
      "batch 435 g_loss : 1.155920\n",
      "batch 436 d_loss : 0.470471\n",
      "batch 436 g_loss : 1.151428\n",
      "batch 437 d_loss : 0.493984\n",
      "batch 437 g_loss : 1.125602\n",
      "batch 438 d_loss : 0.542850\n",
      "batch 438 g_loss : 1.086876\n",
      "batch 439 d_loss : 0.516371\n",
      "batch 439 g_loss : 1.065083\n",
      "batch 440 d_loss : 0.499298\n",
      "batch 440 g_loss : 0.998626\n",
      "batch 441 d_loss : 0.556622\n",
      "batch 441 g_loss : 1.013063\n",
      "batch 442 d_loss : 0.534260\n",
      "batch 442 g_loss : 0.896984\n",
      "batch 443 d_loss : 0.534990\n",
      "batch 443 g_loss : 1.010226\n",
      "batch 444 d_loss : 0.505278\n",
      "batch 444 g_loss : 1.017868\n",
      "batch 445 d_loss : 0.539893\n",
      "batch 445 g_loss : 1.019863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 446 d_loss : 0.500627\n",
      "batch 446 g_loss : 1.052918\n",
      "batch 447 d_loss : 0.516877\n",
      "batch 447 g_loss : 1.033765\n",
      "batch 448 d_loss : 0.540100\n",
      "batch 448 g_loss : 1.025306\n",
      "batch 449 d_loss : 0.515256\n",
      "batch 449 g_loss : 1.032378\n",
      "batch 450 d_loss : 0.518889\n",
      "batch 450 g_loss : 0.969863\n",
      "batch 451 d_loss : 0.494298\n",
      "batch 451 g_loss : 0.979168\n",
      "batch 452 d_loss : 0.523121\n",
      "batch 452 g_loss : 1.032262\n",
      "batch 453 d_loss : 0.515570\n",
      "batch 453 g_loss : 1.041769\n",
      "batch 454 d_loss : 0.513583\n",
      "batch 454 g_loss : 1.017233\n",
      "batch 455 d_loss : 0.553459\n",
      "batch 455 g_loss : 0.996767\n",
      "batch 456 d_loss : 0.527875\n",
      "batch 456 g_loss : 1.029938\n",
      "batch 457 d_loss : 0.503970\n",
      "batch 457 g_loss : 1.034992\n",
      "batch 458 d_loss : 0.458707\n",
      "batch 458 g_loss : 1.031411\n",
      "batch 459 d_loss : 0.501081\n",
      "batch 459 g_loss : 1.080558\n",
      "batch 460 d_loss : 0.492093\n",
      "batch 460 g_loss : 1.030357\n",
      "batch 461 d_loss : 0.483584\n",
      "batch 461 g_loss : 1.068798\n",
      "batch 462 d_loss : 0.504948\n",
      "batch 462 g_loss : 1.074921\n",
      "batch 463 d_loss : 0.529442\n",
      "batch 463 g_loss : 1.054476\n",
      "batch 464 d_loss : 0.592618\n",
      "batch 464 g_loss : 1.126344\n",
      "batch 465 d_loss : 0.528380\n",
      "batch 465 g_loss : 1.077973\n",
      "batch 466 d_loss : 0.563008\n",
      "batch 466 g_loss : 1.009471\n",
      "batch 467 d_loss : 0.555354\n",
      "batch 467 g_loss : 1.040446\n"
     ]
    }
   ],
   "source": [
    "train(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1024, input_dim=100)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080/2560 [=======================>......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "generate(128, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    if args.mode == \"train\":\n",
    "        train(BATCH_SIZE=args.batch_size)\n",
    "    elif args.mode == \"generate\":\n",
    "        generate(BATCH_SIZE=args.batch_size, nice=args.nice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
